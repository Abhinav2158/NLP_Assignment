# -*- coding: utf-8 -*-
"""21004_Abhinav_nlpassignment3_viterbi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkZ3GGG8FHbjkSj1g7jJ_3m8KL1hzpLu
"""

!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/train_data.txt
!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/test_data.txt
!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/noisy_test_data.txt

import numpy as np
from collections import defaultdict, Counter


def load_data(file_path):
    data = []
    with open(file_path, 'r') as file:
        for line in file:
            sentence = []
            for token in line.strip().split():
                word, tag = token.rsplit('/', 1)
                sentence.append((word.lower(), tag))
            data.append(sentence)
    return data

train_data_file = '/content/train_data.txt'
test_data_file = '/content/test_data.txt'
noisy_test_data_file = '/content/noisy_test_data.txt'

train_data = load_data(train_data_file)
test_data = load_data(test_data_file)
noisy_test_data = load_data(noisy_test_data_file)


tags = set()
tag_transitions = defaultdict(Counter)
emissions = defaultdict(Counter)
tag_counts = Counter()

for sentence in train_data:
    prev_tag = "<START>"
    for word, tag in sentence:
        tags.add(tag)
        tag_transitions[prev_tag][tag] += 1
        emissions[tag][word] += 1
        tag_counts[tag] += 1
        prev_tag = tag
    tag_transitions[prev_tag]["<END>"] += 1


def calculate_probabilities():
    transition_probs = {tag: {} for tag in tag_transitions}
    emission_probs = {tag: {} for tag in emissions}

    for tag, transitions in tag_transitions.items():
        total = sum(transitions.values())
        for next_tag, count in transitions.items():
            transition_probs[tag][next_tag] = count / total

    for tag, words in emissions.items():
        total = tag_counts[tag]
        for word, count in words.items():
            emission_probs[tag][word] = count / total

    return transition_probs, emission_probs

transition_probs, emission_probs = calculate_probabilities()


def viterbi_algorithm(sentence, transition_probs, emission_probs, tags, noise_handling=False):
    n = len(sentence)
    viterbi = [{}]
    backpointer = [{}]

    for tag in tags:
        viterbi[0][tag] = transition_probs.get("<START>", {}).get(tag, 0) * emission_probs[tag].get(sentence[0], 1e-6)
        backpointer[0][tag] = None


    for t in range(1, n):
        viterbi.append({})
        backpointer.append({})
        for tag in tags:
            max_tr_prob, best_prev_tag = max(
                (viterbi[t-1][prev_tag] * transition_probs.get(prev_tag, {}).get(tag, 1e-6), prev_tag)
                for prev_tag in tags
            )
            max_prob = max_tr_prob * emission_probs[tag].get(sentence[t], 1e-6)
            if noise_handling:

                  max_prob *= 0.9 if sentence[t] not in emission_probs[tag] else 1.1
            viterbi[t][tag] = max_prob
            backpointer[t][tag] = best_prev_tag

    best_path_prob, best_last_tag = max(
        (viterbi[n-1][tag] * transition_probs.get(tag, {}).get("<END>", 1e-6), tag) for tag in tags
    )


    best_path = [best_last_tag]
    for t in range(n-1, 0, -1):
        best_path.insert(0, backpointer[t][best_path[0]])

    return best_path, best_path_prob


def evaluate(test_data, transition_probs, emission_probs, tags, noise_handling=False):
    correct, total = 0, 0
    for sentence in test_data:
        words = [word for word, _ in sentence]
        actual_tags = [tag for _, tag in sentence]
        predicted_tags, _ = viterbi_algorithm(words, transition_probs, emission_probs, tags, noise_handling)
        correct += sum(p == a for p, a in zip(predicted_tags, actual_tags))
        total += len(actual_tags)
    return correct / total

"""###Evaluation

"""

# Performance comparison
accuracy_baseline = evaluate(test_data, transition_probs, emission_probs, tags, noise_handling=False)
accuracy_noise_handling = evaluate(noisy_test_data, transition_probs, emission_probs, tags, noise_handling=True)

print(f"Baseline Viterbi Accuracy: {accuracy_baseline* 100:.2f}%")
print(f"Viterbi with Noise Handling Accuracy: {accuracy_noise_handling* 100:.2f}%")



#sample_sentence = [word for word, _ in noisy_test_data[0]]
#predicted_tags, _ = viterbi_algorithm(sample_sentence, transition_probs, emission_probs, tags, noise_handling=True)
#print("Sample sentence:", sample_sentence)
#print("Predicted tags:", predicted_tags)

