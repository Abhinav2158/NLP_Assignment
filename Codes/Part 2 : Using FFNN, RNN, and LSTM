# -*- coding: utf-8 -*-
"""21004_Abhinav_nlpassignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1em_ldbCD-t-o22h6lHdXv4KoInUoMkuB

### NLP Assignment-2

Load data
"""

!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/train_split.csv
!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/test_split.csv

import pandas as pd
import numpy as np
import re
import nltk
from nltk.tokenize import word_tokenize
from collections import defaultdict
import random
from sklearn.preprocessing import OneHotEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, RNN, Flatten, Bidirectional
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
nltk.download('punkt_tab')

train_data = pd.read_csv('train_split.csv')
test_data = pd.read_csv('test_split.csv')

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    return tokens

train_data['tokens'] = train_data['text'].apply(preprocess_text)

#  Now Building the Vocabulary and mapping each word with unique index
vocab = set()
for tokens in train_data['tokens']:
    vocab.update(tokens)

word_to_index = {word: idx for idx, word in enumerate(vocab)}
index_to_word = {idx: word for word, idx in word_to_index.items()}
vocab_size = len(vocab)

#   generating Skip-gram Data :-
def generate_skipgram_data(tokens, window_size=2, num_negative_samples=5):
    X, y = [], []
    for i, word in enumerate(tokens):
        target_word = word_to_index[word]
        context_words = tokens[max(0, i - window_size): i] + tokens[i + 1: min(len(tokens), i + window_size + 1)]

        for context_word in context_words:
            X.append((target_word, word_to_index[context_word], 1))
        negative_samples = random.sample(list(vocab - set(context_words)), num_negative_samples)
        for negative_word in negative_samples:
            X.append((target_word, word_to_index[negative_word], 0))

    return X

training_data = []
for tokens in train_data['tokens']:
    training_data += generate_skipgram_data(tokens)


#   Word2Vec Model with Negative Sampling:-
class Word2VecWithNegativeSampling:
    def __init__(self, vocab_size, embedding_size=100):
        self.vocab_size = vocab_size
        self.embedding_size = embedding_size
        self.W1 = np.random.randn(vocab_size, embedding_size) * 0.01
        self.W2 = np.random.randn(vocab_size, embedding_size) * 0.01

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, target_word_indices, context_word_indices):
        h = self.W1[target_word_indices]
        u = self.W2[context_word_indices]
        score = np.sum(h * u, axis=1)
        return self.sigmoid(score)

    def backward(self, target_word_indices, context_word_indices, labels, learning_rate=0.01):
        y_pred = self.forward(target_word_indices, context_word_indices)
        error = y_pred - labels

        for i, target_idx in enumerate(target_word_indices):
            self.W1[target_idx] -= learning_rate * error[i] * self.W2[context_word_indices[i]]
        for i, context_idx in enumerate(context_word_indices):
            self.W2[context_idx] -= learning_rate * error[i] * self.W1[target_word_indices[i]]

    def train(self, data, epochs, learning_rate=0.01, batch_size=64):
        for epoch in range(epochs):
            total_loss = 0
            np.random.shuffle(data)

            for i in range(0, len(data), batch_size):
                batch = data[i:i + batch_size]
                target_indices = np.array([sample[0] for sample in batch])
                context_indices = np.array([sample[1] for sample in batch])
                labels = np.array([sample[2] for sample in batch])

                self.backward(target_indices, context_indices, labels, learning_rate)

                pred = self.forward(target_indices, context_indices)
                loss = -labels * np.log(pred + 1e-9) - (1 - labels) * np.log(1 - pred + 1e-9)
                total_loss += np.sum(loss)

            print(f'Epoch {epoch + 1}, Loss: {total_loss}')

# word2vec = Word2VecWithNegativeSampling(vocab_size=vocab_size, embedding_size=100)
# word2vec.train(training_data, epochs=50, learning_rate=0.01)
# word_embeddings = word2vec.W1
# np.save('word_embeddings.npy', word_embeddings)

"""## Modeling :-"""

!wget https://github.com/Abhinav2158/NLP_Assignment/raw/refs/heads/main/word_embeddings.npy
!wget https://github.com/Abhinav2158/NLP_Assignment/raw/refs/heads/main/model_weights.weights.h5

word_embeddings = np.load('/content/word_embeddings.npy', allow_pickle=True)
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    return tokens

train_data['tokens'] = train_data['text'].apply(preprocess_text)
train_data['tokens'] = train_data['tokens'].apply(lambda tokens: [word_to_index[word] for word in tokens if word in word_to_index])

max_sequence_length = 128
X = pad_sequences(train_data['tokens'], maxlen =  max_sequence_length)
y = train_data[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']].values
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, y_train = X, y
embedding_size = min(100, word_embeddings.shape[1])

model = Sequential()

# Embedding Layer (using pretrained embeddings)
model.add(Embedding(input_dim=word_embeddings.shape[0],
                    output_dim=embedding_size, weights=[word_embeddings],
                    input_length=max_sequence_length, trainable=False))


model.add(Bidirectional(LSTM(64, return_sequences=False)))
model.add(Dense(64, activation='relu'))
model.add(Dense(5, activation='sigmoid'))  # 5 output units for 5 labels


optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])


# history = model.fit(X_train, y_train, epochs=50, batch_size=32)
# model.save('model_weights.weights.h5')

import numpy as np
import tensorflow as tf
from sklearn.metrics import f1_score
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences



word_embeddings = np.load('/content/word_embeddings.npy', allow_pickle=True)
model = tf.keras.models.load_model('/content/model_weights.weights.h5')

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    return tokens

test_data['tokens'] = test_data['text'].apply(preprocess_text)
test_data['tokens'] = test_data['tokens'].apply(lambda tokens: [word_to_index[word] for word in tokens if word in word_to_index])

# Prepare the sequences
X_test = pad_sequences(test_data['tokens'], maxlen=128)

# Step 3: Make predictions
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions

# Step 4: Calculate Macro F1 Score
y_test = test_data[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']].values  # Assuming you have the true labels
macro_f1 = f1_score(y_test, y_pred, average='macro')
print(f'Macro F1 Score: {macro_f1:.3f}')
